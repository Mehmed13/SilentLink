{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T02:42:39.011695Z",
     "iopub.status.busy": "2024-12-12T02:42:39.011231Z",
     "iopub.status.idle": "2024-12-12T02:42:39.045366Z",
     "shell.execute_reply": "2024-12-12T02:42:39.044470Z",
     "shell.execute_reply.started": "2024-12-12T02:42:39.011650Z"
    },
    "id": "_JhhKcssNpxa",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tqdm import tqdm\n",
    "class SentenceGenerator:\n",
    "    def __init__(self, max_vocab_size=10000, max_input_length=10, max_output_length=30):\n",
    "        \"\"\"\n",
    "        Initialize the Sentence Generator Model\n",
    "\n",
    "        Parameters:\n",
    "        - max_vocab_size: Maximum number of words to keep in the vocabulary\n",
    "        - max_input_length: Maximum length of input word sequence\n",
    "        - max_output_length: Maximum length of generated sentence\n",
    "        \"\"\"\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "\n",
    "        # Special tokens\n",
    "        self.start_token = '<start>'\n",
    "        self.end_token = '<end>'\n",
    "        self.pad_token = '<pad>'\n",
    "\n",
    "        # Tokenizers for input and output\n",
    "        self.input_tokenizer = Tokenizer(num_words=max_vocab_size, oov_token='<OOV>')\n",
    "        self.output_tokenizer = Tokenizer(\n",
    "            num_words=max_vocab_size,\n",
    "            oov_token='<OOV>',\n",
    "            filters='',  # Keep all characters\n",
    "            lower=False  # Preserve case\n",
    "        )\n",
    "\n",
    "        # Model components\n",
    "        self.encoder_model = None\n",
    "        self.decoder_model = None\n",
    "        self.model = None\n",
    "\n",
    "    def prepare_data(self, input_sequences, output_sentences):\n",
    "        \"\"\"\n",
    "        Prepare training data by tokenizing and padding sequences\n",
    "\n",
    "        Parameters:\n",
    "        - input_sequences: List of input word sequences\n",
    "        - output_sentences: Corresponding full sentences\n",
    "\n",
    "        Returns:\n",
    "        - Prepared input and output sequences\n",
    "        \"\"\"\n",
    "        # Add special tokens to output sentences if not already present\n",
    "        processed_sentences = []\n",
    "        for sent in output_sentences:\n",
    "            if not sent.startswith(self.start_token):\n",
    "                sent = f\"{self.start_token} {sent}\"\n",
    "            if not sent.endswith(self.end_token):\n",
    "                sent = f\"{sent} {self.end_token}\"\n",
    "            processed_sentences.append(sent)\n",
    "\n",
    "        # Fit tokenizers\n",
    "        self.input_tokenizer.fit_on_texts(input_sequences)\n",
    "        self.output_tokenizer.fit_on_texts(processed_sentences)\n",
    "\n",
    "        # Ensure special tokens are in the word index\n",
    "        if self.start_token not in self.output_tokenizer.word_index:\n",
    "            self.output_tokenizer.word_index[self.start_token] = len(self.output_tokenizer.word_index) + 1\n",
    "        if self.end_token not in self.output_tokenizer.word_index:\n",
    "            self.output_tokenizer.word_index[self.end_token] = len(self.output_tokenizer.word_index) + 1\n",
    "\n",
    "        # Convert to sequences\n",
    "        input_seq = self.input_tokenizer.texts_to_sequences(input_sequences)\n",
    "        output_seq = self.output_tokenizer.texts_to_sequences(processed_sentences)\n",
    "\n",
    "        # Pad sequences\n",
    "        input_pad = pad_sequences(input_seq, maxlen=self.max_input_length, padding='post')\n",
    "        output_pad = pad_sequences(output_seq, maxlen=self.max_output_length, padding='post')\n",
    "\n",
    "        return input_pad, output_pad\n",
    "\n",
    "    def build_model(self, input_vocab_size, output_vocab_size, embedding_dim=256, units=512):\n",
    "        \"\"\"\n",
    "        Build the sequence-to-sequence model with attention\n",
    "\n",
    "        Parameters:\n",
    "        - input_vocab_size: Size of input vocabulary\n",
    "        - output_vocab_size: Size of output vocabulary\n",
    "        - embedding_dim: Dimension of embedding layer\n",
    "        - units: Number of LSTM units\n",
    "        \"\"\"\n",
    "        # Encoder\n",
    "        encoder_inputs = Input(shape=(self.max_input_length,))\n",
    "        encoder_embedding = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n",
    "        encoder = LSTM(units, return_sequences=True, return_state=True)\n",
    "        encoder_outputs, state_h, state_c = encoder(encoder_embedding)\n",
    "        encoder_states = [state_h, state_c]\n",
    "\n",
    "        # Decoder\n",
    "        decoder_inputs = Input(shape=(self.max_output_length,))\n",
    "        decoder_embedding = Embedding(output_vocab_size, embedding_dim)(decoder_inputs)\n",
    "        decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
    "\n",
    "        # Attention Layer\n",
    "        attention_layer = Attention()\n",
    "\n",
    "        # Decoder outputs\n",
    "        decoder_lstm_output, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "        # Apply attention\n",
    "        context_vector = attention_layer([decoder_lstm_output, encoder_outputs])\n",
    "\n",
    "        # Final dense layer\n",
    "        decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "        decoder_outputs = decoder_dense(context_vector)\n",
    "\n",
    "        # Compile model\n",
    "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def train(self, input_sequences, output_sentences, epochs=3, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the sequence-to-sentence model\n",
    "\n",
    "        Parameters:\n",
    "        - input_sequences: Training input word sequences\n",
    "        - output_sentences: Corresponding training sentences\n",
    "        - epochs: Number of training epochs\n",
    "        - batch_size: Batch size for training\n",
    "        \"\"\"\n",
    "        # Prepare data\n",
    "        X_input, y_output = self.prepare_data(input_sequences, output_sentences)\n",
    "\n",
    "        # Get vocabulary sizes\n",
    "        input_vocab_size = len(self.input_tokenizer.word_index) + 1\n",
    "        output_vocab_size = len(self.output_tokenizer.word_index) + 1\n",
    "\n",
    "        # Build model\n",
    "        self.build_model(input_vocab_size, output_vocab_size)\n",
    "\n",
    "        # Prepare decoder target data (shifted by one timestep)\n",
    "        decoder_target_data = np.zeros_like(y_output)\n",
    "        decoder_target_data[:, :-1] = y_output[:, 1:]\n",
    "\n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            [X_input, y_output],\n",
    "            decoder_target_data,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.2\n",
    "        )\n",
    "        return history\n",
    "\n",
    "    def generate_sentence(self, input_words):\n",
    "        \"\"\"\n",
    "        Generate a sentence from input words\n",
    "\n",
    "        Parameters:\n",
    "        - input_words: List of input words\n",
    "\n",
    "        Returns:\n",
    "        - Generated sentence\n",
    "        \"\"\"\n",
    "        # Convert input words to sequence\n",
    "        input_seq = self.input_tokenizer.texts_to_sequences([input_words])\n",
    "        input_seq = pad_sequences(input_seq, maxlen=self.max_input_length, padding='post')\n",
    "\n",
    "        # Ensure start token is in the word index\n",
    "        if self.start_token not in self.output_tokenizer.word_index:\n",
    "            self.output_tokenizer.word_index[self.start_token] = len(self.output_tokenizer.word_index) + 1\n",
    "\n",
    "        # Predict sentence\n",
    "        predicted_sequence = np.zeros((1, self.max_output_length))\n",
    "        predicted_sequence[0, 0] = self.output_tokenizer.word_index[self.start_token]\n",
    "\n",
    "        for i in range(1, self.max_output_length):\n",
    "            decoder_input = predicted_sequence[:, :i]\n",
    "            outputs = self.model.predict([input_seq, decoder_input], verbose=0)\n",
    "            predicted_word_index = np.argmax(outputs[0, -1, :])\n",
    "\n",
    "            predicted_sequence[0, i] = predicted_word_index\n",
    "\n",
    "            # Stop if end token or max length reached\n",
    "            if (predicted_word_index == self.output_tokenizer.word_index.get(self.end_token, -1)) or (i == self.max_output_length - 1):\n",
    "                break\n",
    "\n",
    "        # Convert back to words\n",
    "        generated_words = []\n",
    "        for idx in predicted_sequence[0]:\n",
    "            if idx > 0:\n",
    "                word = self.output_tokenizer.index_word.get(idx, '')\n",
    "                if word and word not in [self.start_token, self.end_token]:\n",
    "                    generated_words.append(word)\n",
    "\n",
    "        return ' '.join(generated_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T00:38:54.633743Z",
     "iopub.status.busy": "2024-12-12T00:38:54.633374Z",
     "iopub.status.idle": "2024-12-12T00:38:55.797443Z",
     "shell.execute_reply": "2024-12-12T00:38:55.796403Z",
     "shell.execute_reply.started": "2024-12-12T00:38:54.633710Z"
    },
    "id": "ok3V2z2yNtvk",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path_to_data = '/kaggle/input/nlp-dataset/seq2seq_data.csv'\n",
    "data = pd.read_csv(path_to_data)\n",
    "input_sentences = data['base_word_text'].tolist()\n",
    "output_sentences = data['original_text'].tolist()\n",
    "input_sentences = [str(sentence) for sentence in input_sentences]\n",
    "input_sentences = [sentence.split() for sentence in input_sentences]\n",
    "output_sentences = [str(sentence) for sentence in output_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T00:38:55.799131Z",
     "iopub.status.busy": "2024-12-12T00:38:55.798855Z",
     "iopub.status.idle": "2024-12-12T02:36:38.122233Z",
     "shell.execute_reply": "2024-12-12T02:36:38.121316Z",
     "shell.execute_reply.started": "2024-12-12T00:38:55.799106Z"
    },
    "id": "8x2HrAIYNw-U",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m4368/4368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2352s\u001b[0m 538ms/step - accuracy: 0.6364 - loss: 2.8989 - val_accuracy: 0.8344 - val_loss: 1.1182\n",
      "Epoch 2/3\n",
      "\u001b[1m4368/4368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2351s\u001b[0m 538ms/step - accuracy: 0.7316 - loss: 1.7451 - val_accuracy: 0.8593 - val_loss: 0.8460\n",
      "Epoch 3/3\n",
      "\u001b[1m4368/4368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2352s\u001b[0m 538ms/step - accuracy: 0.7576 - loss: 1.4216 - val_accuracy: 0.8783 - val_loss: 0.6482\n"
     ]
    }
   ],
   "source": [
    "input_sequences = input_sentences\n",
    "output_sentence = output_sentences\n",
    "\n",
    "# Create and train model\n",
    "model = SentenceGenerator()\n",
    "history = model.train(input_sequences, output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:22:30.908242Z",
     "iopub.status.busy": "2024-12-12T03:22:30.907811Z",
     "iopub.status.idle": "2024-12-12T03:22:31.331815Z",
     "shell.execute_reply": "2024-12-12T03:22:31.331059Z",
     "shell.execute_reply.started": "2024-12-12T03:22:30.908205Z"
    },
    "id": "FePOYCJSN643",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_to_data)\n",
    "sample_data = df.sample(n=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:22:31.355735Z",
     "iopub.status.busy": "2024-12-12T03:22:31.355437Z",
     "iopub.status.idle": "2024-12-12T03:26:16.012911Z",
     "shell.execute_reply": "2024-12-12T03:26:16.011974Z",
     "shell.execute_reply.started": "2024-12-12T03:22:31.355708Z"
    },
    "id": "YHuom886OKPY",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [03:44<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy exact match: 0.00%\n",
      "F1-Score: 45.33%\n",
      "Precision: 59.84%\n",
      "Recall: 38.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_f1(predicted, ground_truth):\n",
    "    predicted_tokens = set(predicted.lower().split())\n",
    "    ground_truth_tokens = set(ground_truth.lower().split())\n",
    "\n",
    "    common = predicted_tokens.intersection(ground_truth_tokens)\n",
    "    if len(common) == 0:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    precision = len(common) / len(predicted_tokens)\n",
    "    recall = len(common) / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "def exact_match(predicted, ground_truth):\n",
    "    return 1 if predicted.lower() == ground_truth.lower() else 0\n",
    "    \n",
    "# define evaluation function\n",
    "def evaluate_deep_learning(dataset, model):\n",
    "  input_base_word = dataset['base_word_text']\n",
    "  ground_truth = dataset['original_text']\n",
    "  ground_truth = [str(gt) for gt in ground_truth]\n",
    "  input_base_word = [str(sentence) for sentence in input_base_word]\n",
    "  input_base_word = [sentence.split() for sentence in input_base_word]\n",
    "  predicted = []\n",
    "\n",
    "  for base_word in tqdm(input_base_word):\n",
    "    predicted.append(str(model.generate_sentence(base_word)))\n",
    "\n",
    "  exact_match_scores = [exact_match(pred, gt) for pred, gt in zip(predicted, ground_truth)]\n",
    "  exact_match_result = sum(exact_match_scores) / len(exact_match_scores)\n",
    "\n",
    "  f1_scores = [compute_f1(pred, gt) for pred, gt in zip(predicted, ground_truth)]\n",
    "  precision_scores = [score[0] for score in f1_scores]\n",
    "  recall_scores = [score[1] for score in f1_scores]\n",
    "  f1_scores = [score[2] for score in f1_scores]\n",
    "\n",
    "  precision_result = sum(precision_scores) / len(precision_scores)\n",
    "  recall_result = sum(recall_scores) / len(recall_scores)\n",
    "  f1_result = sum(f1_scores) / len(f1_scores)\n",
    "  print(f\"Accuracy exact match: {exact_match_result:.2%}\")\n",
    "  print(f\"F1-Score: {f1_result:.2%}\")\n",
    "  print(f\"Precision: {precision_result:.2%}\")\n",
    "  print(f\"Recall: {recall_result:.2%}\")    \n",
    "    \n",
    "evaluate_deep_learning(sample_data, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T04:16:14.919674Z",
     "iopub.status.busy": "2024-12-12T04:16:14.919306Z",
     "iopub.status.idle": "2024-12-12T04:16:39.674451Z",
     "shell.execute_reply": "2024-12-12T04:16:39.673529Z",
     "shell.execute_reply.started": "2024-12-12T04:16:14.919642Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['how', 'to', 'lose', 'fat', 'fast', '1', 'aerobics', '2', 'weight', 'training', '3', 'both']\n",
      "output: how can i lose a <OOV> weight weight weight weight weight <OOV>\n",
      "ground truth: how to lose fat fast? 1) aerobics 2) weight training 3) both?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:01,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['you', 'think', 'that', 'wojtek', 'wolski', 'on', 'colorado', 'avalanche', 'this', 'year']\n",
      "output: do you think that that <OOV> <OOV> <OOV> this <OOV> this this year?\n",
      "ground truth: do you think that wojtek wolski should have been on the colorado avalanche this year?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:03,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['shah', 'rukh', 'khan', 'born', 'to', 'muslim', 'family', 'and', 'married', 'to', 'muslim', 'gauri', 'chibber', 'who', 'later', 'converted', 'to', 'islam']\n",
      "output: declarative: to <OOV> <OOV> <OOV> <OOV> to <OOV> <OOV> to <OOV> <OOV> to <OOV> <OOV> to <OOV> <OOV> to <OOV> <OOV> to <OOV> <OOV>\n",
      "ground truth:  declarative: shah rukh khan was born to a muslim family and married to a muslim, gauri chibber, who later converted to islam.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:03,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['child', 'in', 'washington', 'dc', 'must', '5', 'year', 'old', 'and', 'meet', 'cutoff', 'date', 'of', 'february', '6th', 'to', 'start', 'kindergarten']\n",
      "output: declarative: you can get a <OOV> <OOV> and <OOV> <OOV> to start <OOV> <OOV>\n",
      "ground truth:  children in washington d.c. must be 5 years old and meet the cutoff date of february 6th to start kindergarten.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:04,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['jesus', 'giving', 'you', 'free', 'gift', 'why', 'not', 'take', 'it']\n",
      "output: declarative: you is you <OOV> <OOV> <OOV> not it <OOV>\n",
      "ground truth: jesus is giving you a free gift, why not take it?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:05,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['share', 'information', 'about', 'lowest', 'paid', 'ceo', 'in', 'world']\n",
      "output: imperative: find the about information about the <OOV> <OOV> in the world!\n",
      "ground truth: imperative: share information about the lowest paid ceo in the world! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:07,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['improving', 'geograph', 'and', 'history', 'requires', 'consistent', 'reading', 'and', 'exploring', 'various', 'historical', 'event']\n",
      "output: declarative: the <OOV> and <OOV> requires a <OOV> <OOV> and <OOV> and many historical historical historical historical historical historical historical historical historical <OOV> historical historical historical historical historical historical\n",
      "ground truth:  declarative: improving geograph and history requires consistent reading and exploring various historical events.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:07,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['solve', 'trig', 'question', 'for', 'me']\n",
      "output: imperative: check the <OOV> <OOV> for me!\n",
      "ground truth: imperative: solve trig question for me! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:07,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['tell', 'me', 'about', 'possibility']\n",
      "output: tell me about a <OOV>\n",
      "ground truth: tell me about the possibility! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:08,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['this', 'energy', 'absorbed', 'in', 'each', 'cycle', 'and', 'time', 'a', 'magnetic', 'hysteresis', 'loss']\n",
      "output: declarative: <OOV> <OOV> in the <OOV> <OOV> in the <OOV> <OOV> <OOV> <OOV>\n",
      "ground truth:  declarative: this energy is absorbed in each cycle and time as magnetic hysteresis loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:09,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['why', 'illegal', 'hispanic', 'hate', 'american']\n",
      "output: why why why americans do <OOV>\n",
      "ground truth: why do illegal hispanics hate americans?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:10,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['end', 'of', 'world', 'not', 'predicted', 'to', 'occur', 'on', 'december', '21', '2012']\n",
      "output: declarative: the <OOV> of the <OOV> is not not not not not due of <OOV> <OOV>\n",
      "ground truth:  declarative: the end of the world was not predicted to occur on december 21, 2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:11,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['why', 'yahoo', 'nt', 'crawl', 'my', 'site', 'i', 'submited', 'it', 'one', 'week', 'now', 'url', 'http', 'wwwcamcorderbatterybankcom']\n",
      "output: i have one <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
      "ground truth: why yahoo! haven't crawl my site? i have submited it one week now! url:http:www.camcorder-battery-bank.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:11,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['you', 'consult', 'doctor', 'immediately', 'a', 'you', 're', 'experiencing', 'excruciating', 'pain', 'in', 'your', 'jaw', 'and', 'neck']\n",
      "output: declarative: you're experiencing your <OOV> <OOV> in your <OOV> and <OOV>\n",
      "ground truth:  declarative: you should consult a doctor immediately as you're experiencing excruciating pain in your jaw and neck.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:12,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['he', 's', 'playing', 'her', 'and', 'you', 'might', 'need', 'to', 'conversation', 'with', 'him', 'about', 'it']\n",
      "output: declarative: you need to need to a <OOV> <OOV> with a <OOV> with a <OOV> about <OOV>\n",
      "ground truth:  declarative: he's been playing her, and you might need to have a conversation with him about it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:13,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['who', 'can', 'come', 'up', 'with', 'most', 'word', 'fr', 'househome']\n",
      "output: who can be the <OOV> with the first <OOV> <OOV> <OOV>\n",
      "ground truth: who can come up with the most words fr house/home?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:14,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['tell', 'me', 'history', 'of', 'english', 'language']\n",
      "output: tell me the history of english language!\n",
      "ground truth: tell me the history of the english language. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:15,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['horse', 'and', 'donkey', 'long', 'penis', 'due', 'to', 'their', 'evolutionary', 'history', 'and', 'mating', 'habit']\n",
      "output: declarative: a <OOV> <OOV> is a <OOV> to be their <OOV> and <OOV> <OOV>\n",
      "ground truth:   horses and donkeys have long penises due to their evolutionary history and mating habits.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:15,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['explain', 'war', 'on', 'terror', 'to', 'me']\n",
      "output: imperative: explain the war on the war to me!\n",
      "ground truth: imperative: explain the war on terror to me! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:16,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['gloria', 'allred', 'contactable', 'through', 'her', 'law', 'firm']\n",
      "output: declarative: <OOV> <OOV> <OOV> through <OOV> through <OOV>\n",
      "ground truth:  gloria allred is contactable through her law firm.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:17,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['meeting', 'planner', 'app', 'genius', 'meeting', 'gal', 'site', 'at', 'meatpacking', 'district']\n",
      "output: declarative: you can purchase a <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> at <OOV> <OOV>\n",
      "ground truth:  declarative: meeting planner app genius is the meeting gal site at meatpacking district.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:18,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['pentagram', 'on', 'tiara', 'not', 'banish', 'evil', 'christian', 'a', 'it', 'commonly', 'symbol', 'of', 'protection', 'in', 'various', 'culture']\n",
      "output: declarative: it is a <OOV> is it as it is a <OOV> of the <OOV> of various <OOV> in various cultures.\n",
      "ground truth:   declarative: the pentagram on a tiara does not banish evil christians as it is commonly a symbol of protection in various cultures.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:19,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['you', 'talk', 'openly', 'about', 'it', 'and', 'listen', 'with', 'open', 'mind']\n",
      "output: declarative: you should talk about your <OOV> and <OOV> with your <OOV>\n",
      "ground truth:  declarative: you should talk openly about it and listen with an open mind.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:20,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['how', 'animal', 'kestrel', 'deal', 'with', 'problem', 'found', 'in', 'city']\n",
      "output: how <OOV> deal with a problem problems with the <OOV>\n",
      "ground truth: how does the animal kestrel deal with problems found in the city?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:21,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['excel', 'worksheet', 'function', 'for', 'proface', 'prostudio', 'ver', '4', 'used', 'for', 'data', 'analysis', 'and', 'logging', 'operation']\n",
      "output: declarative: <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> and <OOV>\n",
      "ground truth:  the excel worksheet function for proface pro-studio ver 4 is used for data analysis and logging operations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:22,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['what', 'three', 'concept', 'that', 'make', 'up', 'cell', 'theory']\n",
      "output: what that is the three that make a cell cell <OOV>\n",
      "ground truth: what are the three concepts that make up the cell theory?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [00:22,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['why', 'people', 'in', 'this', 'world', 'never', 'satisfied']\n",
      "output: why do people in this world is this <OOV>\n",
      "ground truth: why people in this world never be satisfied?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [00:23,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['when', 'first', 'time', 'you']\n",
      "output: when when when you <OOV>\n",
      "ground truth: when was the first time you...?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [00:24,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['jesus', 'taught', 'importance', 'of', 'offering', 'sacrifice', 'to', 'god', 'a', 'seen', 'in', 'book', 'of', 'malachi', 'and', 'firstcentury', 'jewish', 'practice']\n",
      "output: declarative: the <OOV> <OOV> <OOV> is seen as the <OOV> of <OOV> and <OOV> <OOV>\n",
      "ground truth:  declarative: jesus taught the importance of offering sacrifices to god, as seen in the book of malachi and first-century jewish practice.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:24,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ['there', '2', 'different', 'nlt', 'bible']\n",
      "output: declarative: there is 2 <OOV> <OOV>\n",
      "ground truth: are there 2 different nlt bibles?\n",
      "Accuracy exact match: 0.00%\n",
      "F1-Score: 44.28%\n",
      "Precision: 60.11%\n",
      "Recall: 37.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# define evaluation function\n",
    "def evaluate_deep_learning_manual(dataset, model):\n",
    "  input_base_word = dataset['base_word_text']\n",
    "  ground_truth = dataset['original_text']\n",
    "  ground_truth = [str(gt) for gt in ground_truth]\n",
    "  input_base_word = [str(sentence) for sentence in input_base_word]\n",
    "  input_base_word = [sentence.split() for sentence in input_base_word]\n",
    "  predicted = []\n",
    "\n",
    "  for base_word, gt in tqdm(zip(input_base_word, ground_truth)):\n",
    "    result = str(model.generate_sentence(base_word))\n",
    "    predicted.append(result)\n",
    "    print(\"input:\", base_word)\n",
    "    print(\"output:\", result)\n",
    "    print(\"ground truth:\", gt)\n",
    "\n",
    "  exact_match_scores = [exact_match(pred, gt) for pred, gt in zip(predicted, ground_truth)]\n",
    "  exact_match_result = sum(exact_match_scores) / len(exact_match_scores)\n",
    "\n",
    "  f1_scores = [compute_f1(pred, gt) for pred, gt in zip(predicted, ground_truth)]\n",
    "  precision_scores = [score[0] for score in f1_scores]\n",
    "  recall_scores = [score[1] for score in f1_scores]\n",
    "  f1_scores = [score[2] for score in f1_scores]\n",
    "\n",
    "  precision_result = sum(precision_scores) / len(precision_scores)\n",
    "  recall_result = sum(recall_scores) / len(recall_scores)\n",
    "  f1_result = sum(f1_scores) / len(f1_scores)\n",
    "  print(f\"Accuracy exact match: {exact_match_result:.2%}\")\n",
    "  print(f\"F1-Score: {f1_result:.2%}\")\n",
    "  print(f\"Precision: {precision_result:.2%}\")\n",
    "  print(f\"Recall: {recall_result:.2%}\")    \n",
    "    \n",
    "sample_data = df.sample(n=30, random_state=42)\n",
    "evaluate_deep_learning_manual(sample_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6284649,
     "sourceId": 10175033,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
