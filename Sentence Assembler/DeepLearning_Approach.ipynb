{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10175033,"sourceType":"datasetVersion","datasetId":6284649}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention\nfrom tensorflow.keras.models import Model\nfrom tqdm import tqdm\nclass SentenceGenerator:\n    def __init__(self, max_vocab_size=10000, max_input_length=10, max_output_length=30):\n        \"\"\"\n        Initialize the Sentence Generator Model\n\n        Parameters:\n        - max_vocab_size: Maximum number of words to keep in the vocabulary\n        - max_input_length: Maximum length of input word sequence\n        - max_output_length: Maximum length of generated sentence\n        \"\"\"\n        self.max_vocab_size = max_vocab_size\n        self.max_input_length = max_input_length\n        self.max_output_length = max_output_length\n\n        # Special tokens\n        self.start_token = '<start>'\n        self.end_token = '<end>'\n        self.pad_token = '<pad>'\n\n        # Tokenizers for input and output\n        self.input_tokenizer = Tokenizer(num_words=max_vocab_size, oov_token='<OOV>')\n        self.output_tokenizer = Tokenizer(\n            num_words=max_vocab_size,\n            oov_token='<OOV>',\n            filters='',  # Keep all characters\n            lower=False  # Preserve case\n        )\n\n        # Model components\n        self.encoder_model = None\n        self.decoder_model = None\n        self.model = None\n\n    def prepare_data(self, input_sequences, output_sentences):\n        \"\"\"\n        Prepare training data by tokenizing and padding sequences\n\n        Parameters:\n        - input_sequences: List of input word sequences\n        - output_sentences: Corresponding full sentences\n\n        Returns:\n        - Prepared input and output sequences\n        \"\"\"\n        # Add special tokens to output sentences if not already present\n        processed_sentences = []\n        for sent in output_sentences:\n            if not sent.startswith(self.start_token):\n                sent = f\"{self.start_token} {sent}\"\n            if not sent.endswith(self.end_token):\n                sent = f\"{sent} {self.end_token}\"\n            processed_sentences.append(sent)\n\n        # Fit tokenizers\n        self.input_tokenizer.fit_on_texts(input_sequences)\n        self.output_tokenizer.fit_on_texts(processed_sentences)\n\n        # Ensure special tokens are in the word index\n        if self.start_token not in self.output_tokenizer.word_index:\n            self.output_tokenizer.word_index[self.start_token] = len(self.output_tokenizer.word_index) + 1\n        if self.end_token not in self.output_tokenizer.word_index:\n            self.output_tokenizer.word_index[self.end_token] = len(self.output_tokenizer.word_index) + 1\n\n        # Convert to sequences\n        input_seq = self.input_tokenizer.texts_to_sequences(input_sequences)\n        output_seq = self.output_tokenizer.texts_to_sequences(processed_sentences)\n\n        # Pad sequences\n        input_pad = pad_sequences(input_seq, maxlen=self.max_input_length, padding='post')\n        output_pad = pad_sequences(output_seq, maxlen=self.max_output_length, padding='post')\n\n        return input_pad, output_pad\n\n    def build_model(self, input_vocab_size, output_vocab_size, embedding_dim=256, units=512):\n        \"\"\"\n        Build the sequence-to-sequence model with attention\n\n        Parameters:\n        - input_vocab_size: Size of input vocabulary\n        - output_vocab_size: Size of output vocabulary\n        - embedding_dim: Dimension of embedding layer\n        - units: Number of LSTM units\n        \"\"\"\n        # Encoder\n        encoder_inputs = Input(shape=(self.max_input_length,))\n        encoder_embedding = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n        encoder = LSTM(units, return_sequences=True, return_state=True)\n        encoder_outputs, state_h, state_c = encoder(encoder_embedding)\n        encoder_states = [state_h, state_c]\n\n        # Decoder\n        decoder_inputs = Input(shape=(self.max_output_length,))\n        decoder_embedding = Embedding(output_vocab_size, embedding_dim)(decoder_inputs)\n        decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n\n        # Attention Layer\n        attention_layer = Attention()\n\n        # Decoder outputs\n        decoder_lstm_output, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n\n        # Apply attention\n        context_vector = attention_layer([decoder_lstm_output, encoder_outputs])\n\n        # Final dense layer\n        decoder_dense = Dense(output_vocab_size, activation='softmax')\n        decoder_outputs = decoder_dense(context_vector)\n\n        # Compile model\n        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n        self.model = model\n        return model\n\n    def train(self, input_sequences, output_sentences, epochs=3, batch_size=32):\n        \"\"\"\n        Train the sequence-to-sentence model\n\n        Parameters:\n        - input_sequences: Training input word sequences\n        - output_sentences: Corresponding training sentences\n        - epochs: Number of training epochs\n        - batch_size: Batch size for training\n        \"\"\"\n        # Prepare data\n        X_input, y_output = self.prepare_data(input_sequences, output_sentences)\n\n        # Get vocabulary sizes\n        input_vocab_size = len(self.input_tokenizer.word_index) + 1\n        output_vocab_size = len(self.output_tokenizer.word_index) + 1\n\n        # Build model\n        self.build_model(input_vocab_size, output_vocab_size)\n\n        # Prepare decoder target data (shifted by one timestep)\n        decoder_target_data = np.zeros_like(y_output)\n        decoder_target_data[:, :-1] = y_output[:, 1:]\n\n        # Train model\n        history = self.model.fit(\n            [X_input, y_output],\n            decoder_target_data,\n            epochs=epochs,\n            batch_size=batch_size,\n            validation_split=0.2\n        )\n        return history\n\n    def generate_sentence(self, input_words):\n        \"\"\"\n        Generate a sentence from input words\n\n        Parameters:\n        - input_words: List of input words\n\n        Returns:\n        - Generated sentence\n        \"\"\"\n        # Convert input words to sequence\n        input_seq = self.input_tokenizer.texts_to_sequences([input_words])\n        input_seq = pad_sequences(input_seq, maxlen=self.max_input_length, padding='post')\n\n        # Ensure start token is in the word index\n        if self.start_token not in self.output_tokenizer.word_index:\n            self.output_tokenizer.word_index[self.start_token] = len(self.output_tokenizer.word_index) + 1\n\n        # Predict sentence\n        predicted_sequence = np.zeros((1, self.max_output_length))\n        predicted_sequence[0, 0] = self.output_tokenizer.word_index[self.start_token]\n\n        for i in range(1, self.max_output_length):\n            decoder_input = predicted_sequence[:, :i]\n            outputs = self.model.predict([input_seq, decoder_input], verbose=0)\n            predicted_word_index = np.argmax(outputs[0, -1, :])\n\n            predicted_sequence[0, i] = predicted_word_index\n\n            # Stop if end token or max length reached\n            if (predicted_word_index == self.output_tokenizer.word_index.get(self.end_token, -1)) or (i == self.max_output_length - 1):\n                break\n\n        # Convert back to words\n        generated_words = []\n        for idx in predicted_sequence[0]:\n            if idx > 0:\n                word = self.output_tokenizer.index_word.get(idx, '')\n                if word and word not in [self.start_token, self.end_token]:\n                    generated_words.append(word)\n\n        return ' '.join(generated_words)","metadata":{"id":"_JhhKcssNpxa","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T02:42:39.011231Z","iopub.execute_input":"2024-12-12T02:42:39.011695Z","iopub.status.idle":"2024-12-12T02:42:39.045366Z","shell.execute_reply.started":"2024-12-12T02:42:39.011650Z","shell.execute_reply":"2024-12-12T02:42:39.044470Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\n\npath_to_data = '/kaggle/input/nlp-dataset/seq2seq_data.csv'\ndata = pd.read_csv(path_to_data)\ninput_sentences = data['base_word_text'].tolist()\noutput_sentences = data['original_text'].tolist()\ninput_sentences = [str(sentence) for sentence in input_sentences]\ninput_sentences = [sentence.split() for sentence in input_sentences]\noutput_sentences = [str(sentence) for sentence in output_sentences]","metadata":{"id":"ok3V2z2yNtvk","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T00:38:54.633374Z","iopub.execute_input":"2024-12-12T00:38:54.633743Z","iopub.status.idle":"2024-12-12T00:38:55.797443Z","shell.execute_reply.started":"2024-12-12T00:38:54.633710Z","shell.execute_reply":"2024-12-12T00:38:55.796403Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"input_sequences = input_sentences\noutput_sentence = output_sentences\n\n# Create and train model\nmodel = SentenceGenerator()\nhistory = model.train(input_sequences, output_sentence)","metadata":{"id":"8x2HrAIYNw-U","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T00:38:55.798855Z","iopub.execute_input":"2024-12-12T00:38:55.799131Z","iopub.status.idle":"2024-12-12T02:36:38.122233Z","shell.execute_reply.started":"2024-12-12T00:38:55.799106Z","shell.execute_reply":"2024-12-12T02:36:38.121316Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/3\n\u001b[1m4368/4368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2352s\u001b[0m 538ms/step - accuracy: 0.6364 - loss: 2.8989 - val_accuracy: 0.8344 - val_loss: 1.1182\nEpoch 2/3\n\u001b[1m4368/4368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2351s\u001b[0m 538ms/step - accuracy: 0.7316 - loss: 1.7451 - val_accuracy: 0.8593 - val_loss: 0.8460\nEpoch 3/3\n\u001b[1m4368/4368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2352s\u001b[0m 538ms/step - accuracy: 0.7576 - loss: 1.4216 - val_accuracy: 0.8783 - val_loss: 0.6482\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"df = pd.read_csv(path_to_data)\nsample_data = df.sample(n=300)","metadata":{"id":"FePOYCJSN643","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T03:22:30.907811Z","iopub.execute_input":"2024-12-12T03:22:30.908242Z","iopub.status.idle":"2024-12-12T03:22:31.331815Z","shell.execute_reply.started":"2024-12-12T03:22:30.908205Z","shell.execute_reply":"2024-12-12T03:22:31.331059Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def compute_f1(predicted, ground_truth):\n    predicted_tokens = set(predicted.lower().split())\n    ground_truth_tokens = set(ground_truth.lower().split())\n\n    common = predicted_tokens.intersection(ground_truth_tokens)\n    if len(common) == 0:\n        return 0, 0, 0\n\n    precision = len(common) / len(predicted_tokens)\n    recall = len(common) / len(ground_truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return precision, recall, f1\ndef exact_match(predicted, ground_truth):\n    return 1 if predicted.lower() == ground_truth.lower() else 0\n    \n# define evaluation function\ndef evaluate_deep_learning(dataset, model):\n  input_base_word = dataset['base_word_text']\n  ground_truth = dataset['original_text']\n  ground_truth = [str(gt) for gt in ground_truth]\n  input_base_word = [str(sentence) for sentence in input_base_word]\n  input_base_word = [sentence.split() for sentence in input_base_word]\n  predicted = []\n\n  for base_word in tqdm(input_base_word):\n    predicted.append(str(model.generate_sentence(base_word)))\n\n  exact_match_scores = [exact_match(pred, gt) for pred, gt in zip(predicted, ground_truth)]\n  exact_match_result = sum(exact_match_scores) / len(exact_match_scores)\n\n  f1_scores = [compute_f1(pred, gt) for pred, gt in zip(predicted, ground_truth)]\n  precision_scores = [score[0] for score in f1_scores]\n  recall_scores = [score[1] for score in f1_scores]\n  f1_scores = [score[2] for score in f1_scores]\n\n  precision_result = sum(precision_scores) / len(precision_scores)\n  recall_result = sum(recall_scores) / len(recall_scores)\n  f1_result = sum(f1_scores) / len(f1_scores)\n  print(f\"Accuracy exact match: {exact_match_result:.2%}\")\n  print(f\"F1-Score: {f1_result:.2%}\")\n  print(f\"Precision: {precision_result:.2%}\")\n  print(f\"Recall: {recall_result:.2%}\")    \n    \nevaluate_deep_learning(sample_data, model)","metadata":{"id":"YHuom886OKPY","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T03:22:31.355437Z","iopub.execute_input":"2024-12-12T03:22:31.355735Z","iopub.status.idle":"2024-12-12T03:26:16.012911Z","shell.execute_reply.started":"2024-12-12T03:22:31.355708Z","shell.execute_reply":"2024-12-12T03:26:16.011974Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 300/300 [03:44<00:00,  1.34it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy exact match: 0.00%\nF1-Score: 45.33%\nPrecision: 59.84%\nRecall: 38.56%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# define evaluation function\ndef evaluate_deep_learning_manual(dataset, model):\n  input_base_word = dataset['base_word_text']\n  ground_truth = dataset['original_text']\n  ground_truth = [str(gt) for gt in ground_truth]\n  input_base_word = [str(sentence) for sentence in input_base_word]\n  input_base_word = [sentence.split() for sentence in input_base_word]\n  predicted = []\n\n  for base_word, gt in tqdm(zip(input_base_word, ground_truth)):\n    result = str(model.generate_sentence(base_word))\n    predicted.append(result)\n    print(\"input:\", base_word)\n    print(\"output:\", result)\n    print(\"ground truth:\", gt)\n\n  exact_match_scores = [exact_match(pred, gt) for pred, gt in zip(predicted, ground_truth)]\n  exact_match_result = sum(exact_match_scores) / len(exact_match_scores)\n\n  f1_scores = [compute_f1(pred, gt) for pred, gt in zip(predicted, ground_truth)]\n  precision_scores = [score[0] for score in f1_scores]\n  recall_scores = [score[1] for score in f1_scores]\n  f1_scores = [score[2] for score in f1_scores]\n\n  precision_result = sum(precision_scores) / len(precision_scores)\n  recall_result = sum(recall_scores) / len(recall_scores)\n  f1_result = sum(f1_scores) / len(f1_scores)\n  print(f\"Accuracy exact match: {exact_match_result:.2%}\")\n  print(f\"F1-Score: {f1_result:.2%}\")\n  print(f\"Precision: {precision_result:.2%}\")\n  print(f\"Recall: {recall_result:.2%}\")    \n    \nsample_data = df.sample(n=30, random_state=42)\nevaluate_deep_learning_manual(sample_data, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T04:16:14.919306Z","iopub.execute_input":"2024-12-12T04:16:14.919674Z","iopub.status.idle":"2024-12-12T04:16:39.674451Z","shell.execute_reply.started":"2024-12-12T04:16:14.919642Z","shell.execute_reply":"2024-12-12T04:16:39.673529Z"}},"outputs":[{"name":"stderr","text":"1it [00:00,  1.31it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['how', 'to', 'lose', 'fat', 'fast', '1', 'aerobics', '2', 'weight', 'training', '3', 'both']\noutput: how can i lose a <OOV> weight weight weight weight weight <OOV>\nground truth: how to lose fat fast? 1) aerobics 2) weight training 3) both?\n","output_type":"stream"},{"name":"stderr","text":"2it [00:01,  1.26it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['you', 'think', 'that', 'wojtek', 'wolski', 'on', 'colorado', 'avalanche', 'this', 'year']\noutput: do you think that that <OOV> <OOV> <OOV> this <OOV> this this year?\nground truth: do you think that wojtek wolski should have been on the colorado avalanche this year?\n","output_type":"stream"},{"name":"stderr","text":"3it [00:03,  1.12s/it]","output_type":"stream"},{"name":"stdout","text":"input: ['shah', 'rukh', 'khan', 'born', 'to', 'muslim', 'family', 'and', 'married', 'to', 'muslim', 'gauri', 'chibber', 'who', 'later', 'converted', 'to', 'islam']\noutput: declarative: to <OOV> <OOV> <OOV> <OOV> to <OOV> <OOV> to <OOV> <OOV> to <OOV> <OOV> to <OOV> <OOV> to <OOV> <OOV> to <OOV> <OOV>\nground truth:  declarative: shah rukh khan was born to a muslim family and married to a muslim, gauri chibber, who later converted to islam.\n","output_type":"stream"},{"name":"stderr","text":"4it [00:03,  1.03s/it]","output_type":"stream"},{"name":"stdout","text":"input: ['child', 'in', 'washington', 'dc', 'must', '5', 'year', 'old', 'and', 'meet', 'cutoff', 'date', 'of', 'february', '6th', 'to', 'start', 'kindergarten']\noutput: declarative: you can get a <OOV> <OOV> and <OOV> <OOV> to start <OOV> <OOV>\nground truth:  children in washington d.c. must be 5 years old and meet the cutoff date of february 6th to start kindergarten.\n","output_type":"stream"},{"name":"stderr","text":"5it [00:04,  1.13it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['jesus', 'giving', 'you', 'free', 'gift', 'why', 'not', 'take', 'it']\noutput: declarative: you is you <OOV> <OOV> <OOV> not it <OOV>\nground truth: jesus is giving you a free gift, why not take it?\n","output_type":"stream"},{"name":"stderr","text":"6it [00:05,  1.19it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['share', 'information', 'about', 'lowest', 'paid', 'ceo', 'in', 'world']\noutput: imperative: find the about information about the <OOV> <OOV> in the world!\nground truth: imperative: share information about the lowest paid ceo in the world! \n","output_type":"stream"},{"name":"stderr","text":"7it [00:07,  1.14s/it]","output_type":"stream"},{"name":"stdout","text":"input: ['improving', 'geograph', 'and', 'history', 'requires', 'consistent', 'reading', 'and', 'exploring', 'various', 'historical', 'event']\noutput: declarative: the <OOV> and <OOV> requires a <OOV> <OOV> and <OOV> and many historical historical historical historical historical historical historical historical historical <OOV> historical historical historical historical historical historical\nground truth:  declarative: improving geograph and history requires consistent reading and exploring various historical events.\n","output_type":"stream"},{"name":"stderr","text":"8it [00:07,  1.08it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['solve', 'trig', 'question', 'for', 'me']\noutput: imperative: check the <OOV> <OOV> for me!\nground truth: imperative: solve trig question for me! \n","output_type":"stream"},{"name":"stderr","text":"9it [00:07,  1.35it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['tell', 'me', 'about', 'possibility']\noutput: tell me about a <OOV>\nground truth: tell me about the possibility! \n","output_type":"stream"},{"name":"stderr","text":"10it [00:08,  1.30it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['this', 'energy', 'absorbed', 'in', 'each', 'cycle', 'and', 'time', 'a', 'magnetic', 'hysteresis', 'loss']\noutput: declarative: <OOV> <OOV> in the <OOV> <OOV> in the <OOV> <OOV> <OOV> <OOV>\nground truth:  declarative: this energy is absorbed in each cycle and time as magnetic hysteresis loss.\n","output_type":"stream"},{"name":"stderr","text":"11it [00:09,  1.52it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['why', 'illegal', 'hispanic', 'hate', 'american']\noutput: why why why americans do <OOV>\nground truth: why do illegal hispanics hate americans?\n","output_type":"stream"},{"name":"stderr","text":"12it [00:10,  1.30it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['end', 'of', 'world', 'not', 'predicted', 'to', 'occur', 'on', 'december', '21', '2012']\noutput: declarative: the <OOV> of the <OOV> is not not not not not due of <OOV> <OOV>\nground truth:  declarative: the end of the world was not predicted to occur on december 21, 2012\n","output_type":"stream"},{"name":"stderr","text":"13it [00:11,  1.25it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['why', 'yahoo', 'nt', 'crawl', 'my', 'site', 'i', 'submited', 'it', 'one', 'week', 'now', 'url', 'http', 'wwwcamcorderbatterybankcom']\noutput: i have one <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\nground truth: why yahoo! haven't crawl my site? i have submited it one week now! url:http:www.camcorder-battery-bank.com\n","output_type":"stream"},{"name":"stderr","text":"14it [00:11,  1.31it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['you', 'consult', 'doctor', 'immediately', 'a', 'you', 're', 'experiencing', 'excruciating', 'pain', 'in', 'your', 'jaw', 'and', 'neck']\noutput: declarative: you're experiencing your <OOV> <OOV> in your <OOV> and <OOV>\nground truth:  declarative: you should consult a doctor immediately as you're experiencing excruciating pain in your jaw and neck.\n","output_type":"stream"},{"name":"stderr","text":"15it [00:12,  1.13it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['he', 's', 'playing', 'her', 'and', 'you', 'might', 'need', 'to', 'conversation', 'with', 'him', 'about', 'it']\noutput: declarative: you need to need to a <OOV> <OOV> with a <OOV> with a <OOV> about <OOV>\nground truth:  declarative: he's been playing her, and you might need to have a conversation with him about it.\n","output_type":"stream"},{"name":"stderr","text":"16it [00:13,  1.17it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['who', 'can', 'come', 'up', 'with', 'most', 'word', 'fr', 'househome']\noutput: who can be the <OOV> with the first <OOV> <OOV> <OOV>\nground truth: who can come up with the most words fr house/home?\n","output_type":"stream"},{"name":"stderr","text":"17it [00:14,  1.35it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['tell', 'me', 'history', 'of', 'english', 'language']\noutput: tell me the history of english language!\nground truth: tell me the history of the english language. \n","output_type":"stream"},{"name":"stderr","text":"18it [00:15,  1.23it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['horse', 'and', 'donkey', 'long', 'penis', 'due', 'to', 'their', 'evolutionary', 'history', 'and', 'mating', 'habit']\noutput: declarative: a <OOV> <OOV> is a <OOV> to be their <OOV> and <OOV> <OOV>\nground truth:   horses and donkeys have long penises due to their evolutionary history and mating habits.\n","output_type":"stream"},{"name":"stderr","text":"19it [00:15,  1.30it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['explain', 'war', 'on', 'terror', 'to', 'me']\noutput: imperative: explain the war on the war to me!\nground truth: imperative: explain the war on terror to me! \n","output_type":"stream"},{"name":"stderr","text":"20it [00:16,  1.42it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['gloria', 'allred', 'contactable', 'through', 'her', 'law', 'firm']\noutput: declarative: <OOV> <OOV> <OOV> through <OOV> through <OOV>\nground truth:  gloria allred is contactable through her law firm.\n","output_type":"stream"},{"name":"stderr","text":"21it [00:17,  1.22it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['meeting', 'planner', 'app', 'genius', 'meeting', 'gal', 'site', 'at', 'meatpacking', 'district']\noutput: declarative: you can purchase a <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> at <OOV> <OOV>\nground truth:  declarative: meeting planner app genius is the meeting gal site at meatpacking district.\n","output_type":"stream"},{"name":"stderr","text":"22it [00:18,  1.01s/it]","output_type":"stream"},{"name":"stdout","text":"input: ['pentagram', 'on', 'tiara', 'not', 'banish', 'evil', 'christian', 'a', 'it', 'commonly', 'symbol', 'of', 'protection', 'in', 'various', 'culture']\noutput: declarative: it is a <OOV> is it as it is a <OOV> of the <OOV> of various <OOV> in various cultures.\nground truth:   declarative: the pentagram on a tiara does not banish evil christians as it is commonly a symbol of protection in various cultures.\n","output_type":"stream"},{"name":"stderr","text":"23it [00:19,  1.04it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['you', 'talk', 'openly', 'about', 'it', 'and', 'listen', 'with', 'open', 'mind']\noutput: declarative: you should talk about your <OOV> and <OOV> with your <OOV>\nground truth:  declarative: you should talk openly about it and listen with an open mind.\n","output_type":"stream"},{"name":"stderr","text":"24it [00:20,  1.12it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['how', 'animal', 'kestrel', 'deal', 'with', 'problem', 'found', 'in', 'city']\noutput: how <OOV> deal with a problem problems with the <OOV>\nground truth: how does the animal kestrel deal with problems found in the city?\n","output_type":"stream"},{"name":"stderr","text":"25it [00:21,  1.09it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['excel', 'worksheet', 'function', 'for', 'proface', 'prostudio', 'ver', '4', 'used', 'for', 'data', 'analysis', 'and', 'logging', 'operation']\noutput: declarative: <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> and <OOV>\nground truth:  the excel worksheet function for proface pro-studio ver 4 is used for data analysis and logging operations.\n","output_type":"stream"},{"name":"stderr","text":"26it [00:22,  1.13it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['what', 'three', 'concept', 'that', 'make', 'up', 'cell', 'theory']\noutput: what that is the three that make a cell cell <OOV>\nground truth: what are the three concepts that make up the cell theory?\n","output_type":"stream"},{"name":"stderr","text":"27it [00:22,  1.22it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['why', 'people', 'in', 'this', 'world', 'never', 'satisfied']\noutput: why do people in this world is this <OOV>\nground truth: why people in this world never be satisfied?\n","output_type":"stream"},{"name":"stderr","text":"28it [00:23,  1.47it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['when', 'first', 'time', 'you']\noutput: when when when you <OOV>\nground truth: when was the first time you...?\n","output_type":"stream"},{"name":"stderr","text":"29it [00:24,  1.26it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['jesus', 'taught', 'importance', 'of', 'offering', 'sacrifice', 'to', 'god', 'a', 'seen', 'in', 'book', 'of', 'malachi', 'and', 'firstcentury', 'jewish', 'practice']\noutput: declarative: the <OOV> <OOV> <OOV> is seen as the <OOV> of <OOV> and <OOV> <OOV>\nground truth:  declarative: jesus taught the importance of offering sacrifices to god, as seen in the book of malachi and first-century jewish practice.\n","output_type":"stream"},{"name":"stderr","text":"30it [00:24,  1.21it/s]","output_type":"stream"},{"name":"stdout","text":"input: ['there', '2', 'different', 'nlt', 'bible']\noutput: declarative: there is 2 <OOV> <OOV>\nground truth: are there 2 different nlt bibles?\nAccuracy exact match: 0.00%\nF1-Score: 44.28%\nPrecision: 60.11%\nRecall: 37.26%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}